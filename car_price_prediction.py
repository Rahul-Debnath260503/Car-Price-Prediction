# -*- coding: utf-8 -*-
"""Car Price Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vMaFBLFBuVWxtkrkjvTHlWghOJYdd66K

# importing libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""# load data"""

df = pd.read_csv("/content/car_details.csv")

"""# EDA & Data cleaning"""

df.head()



import seaborn as sns

# Select only numerical columns for the pair plot
numerical_df = df.select_dtypes(include=np.number)

# Create a pair plot
sns.pairplot(numerical_df)
plt.suptitle("Pair Plot of Numerical Features", y=1.02)
plt.show()

df.describe()

df.columns

df.isnull().sum()

df.dropna(inplace=True)

df.isnull().sum()

df.dtypes

numerical_cols = df.select_dtypes(include=np.number).columns

for col in numerical_cols:
    plt.figure(figsize=(10, 6))
    plt.boxplot(df[col])
    plt.title(f'Box plot of {col}')
    plt.ylabel(col)
    plt.show()

numerical_cols = df.select_dtypes(include=np.number).columns

for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    print(f"Outliers in column '{col}':")
    display(outliers)

numerical_cols = df.select_dtypes(include=np.number).columns

for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]

print("Outliers removed from numerical columns.")

numerical_cols = df.select_dtypes(include=np.number).columns

for col in numerical_cols:
    plt.figure(figsize=(10, 6))
    plt.boxplot(df[col])
    plt.title(f'Box plot of {col}')
    plt.ylabel(col)
    plt.show()

df.describe()



"""# Encoding coulmns as needed"""

categorical_cols = df.select_dtypes(include='object').columns
print("Categorical columns:")
print(categorical_cols)

df_encoded = pd.get_dummies(df, columns=['fuel', 'seller_type', 'transmission', 'owner'], drop_first=True)
display(df_encoded.head())



"""# Split datasets into train and test split"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Assuming 'selling_price' is your target variable and the rest are features
X = df_encoded.drop(['name', 'max_power', 'selling_price'], axis=1) # Dropping 'name' and 'max_power' as discussed, and the target
y = df_encoded['selling_price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Data split into training and testing sets.")
print("Training features shape:", X_train.shape)
print("Testing features shape:", X_test.shape)
print("Training target shape:", y_train.shape)
print("Testing target shape:", y_test.shape)

"""# Train Random forest model"""

# Train Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

print("Random Forest Regressor model trained.")

"""# Train XG Boost model"""

# Train XGBoost Regressor
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train, y_train)

print("XGBoost Regressor model trained.")

"""# Compare model performance using R² Score, RMSE, and MAE."""



# Make predictions
y_pred_rf = rf_model.predict(X_test)
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate Random Forest
r2_rf = r2_score(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
mae_rf = mean_absolute_error(y_test, y_pred_rf)

print("Random Forest Regressor Performance:")
print(f"R² Score: {r2_rf}")
print(f"RMSE: {rmse_rf}")
print(f"MAE: {mae_rf}")

print("\n" + "="*30 + "\n")

# Evaluate XGBoost
r2_xgb = r2_score(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)

print("XGBoost Regressor Performance:")
print(f"R² Score: {r2_xgb}")
print(f"RMSE: {rmse_xgb}")
print(f"MAE: {mae_xgb}")

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_xgb, alpha=0.5)
plt.xlabel("Actual Selling Price")
plt.ylabel("Predicted Selling Price")
plt.title("Actual vs. Predicted Selling Price (XGBoost)")
plt.grid(True)
plt.show()

"""# Compare model performance using R² Score, RMSE, and MAE."""

metrics = ['R² Score', 'RMSE', 'MAE']
rf_performance = [r2_rf, rmse_rf, mae_rf]
xgb_performance = [r2_xgb, rmse_xgb, mae_xgb]

x = np.arange(len(metrics))
width = 0.35

fig, ax = plt.subplots(figsize=(10, 6))
rects1 = ax.bar(x - width/2, rf_performance, width, label='Random Forest')
rects2 = ax.bar(x + width/2, xgb_performance, width, label='XGBoost')

ax.set_ylabel('Score/Error Value')
ax.set_title('Model Performance Comparison')
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend()

def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.2f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)

fig.tight_layout()
plt.show()



"""# Provide insights on which features most strongly influence car prices."""

# Get feature importances from the XGBoost model
feature_importances = xgb_model.feature_importances_

# Create a pandas Series for easier handling and sorting
feature_importance_series = pd.Series(feature_importances, index=X_train.columns)

# Sort the features by importance
sorted_feature_importance = feature_importance_series.sort_values(ascending=False)

print("Feature Importances (XGBoost):")
display(sorted_feature_importance)

# Visualize feature importances
plt.figure(figsize=(12, 8))
sorted_feature_importance.plot(kind='bar')
plt.title('Feature Importance (XGBoost Regressor)')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""# Car Selling Price Prediction

This project aims to predict the selling price of cars using car_details.csv containing various features such as car name, year, driven distance, fuel type, seller type, transmission, owner type, mileage, engine size, max power, and number of seats.

## Project Steps

1.  **Data Loading**: Loaded the car details dataset into a pandas DataFrame.
2.  **Exploratory Data Analysis (EDA)**:
    *   Displayed the first few rows and basic statistics.
    *   Checked for null values and dropped rows with missing data.
    *   Detected and visualized outliers in numerical columns using box plots and the IQR method.
    *   Removed outliers from numerical columns.
    *   Identified categorical columns.
3.  **Data Preprocessing**:
    *   Performed one-hot encoding on 'fuel', 'seller_type', 'transmission', and 'owner' columns.
    *   Dropped the 'name' and 'max_power' columns for modeling in this iteration.
4.  **Model Training**:
    *   Split the data into training and testing sets (80/20 split).
    *   Trained a **Random Forest Regressor** model.
    *   Trained an **XGBoost Regressor** model.
5.  **Model Evaluation**:
    *   Evaluated both models using R² Score, Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE).
    *   Visualized the performance comparison using a bar plot.
    *   Visualized the actual vs. predicted selling prices for the XGBoost model using a scatter plot.
6.  **Feature Importance**:
    *   Analyzed feature importance from the XGBoost model to understand the influence of different features on selling price.

## Model Performance

The performance of the trained models is as follows:

*   **Random Forest Regressor**:
    *   R² Score: {{r2_rf:.2f}}
    *   RMSE: {{rmse_rf:.2f}}
    *   MAE: {{mae_rf:.2f}}

*   **XGBoost Regressor**:
    *   R² Score: {{r2_xgb:.2f}}
    *   RMSE: {{rmse_xgb:.2f}}
    *   MAE: {{mae_xgb:.2f}}

Based on the R² and RMSE values, the XGBoost Regressor performed slightly better on this dataset.

## Key Feature Insights

The feature importance analysis using the XGBoost model revealed that:

*   **Year** is the most significant factor influencing car selling prices.
*   **Engine size** and **Seller Type (Trustmark Dealer)** are the next most important features.
*   Other features like mileage, transmission, and fuel type also contribute to the prediction.
*   The number of seats did not appear to be a significant predictor after outlier removal.

## Future Work

*   Further explore and potentially engineer features from the 'name' and 'max_power' columns.
*   Perform hyperparameter tuning for the XGBoost model to potentially improve performance.
*   Investigate other regression models.
*   Implement cross-validation for more robust model evaluation.
"""

